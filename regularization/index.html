<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118361649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-118361649-1');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
<link rel="stylesheet" type="text/css" href="css/article.css"/>
<title>A visual explanation of regularization for linear models</title>
<!-- META -->
<!-- LinkedIn meta -->
<meta property='og:title' content="How to explain gradient boosting"/>
<meta property='og:image' content="http://explained.ai/regularization/images/reg3D.svg">
<meta property='og:description' content=""/>
<meta property='og:url' content="http://explained.ai/regularization/index.html"/>

<!-- Facebook meta -->
<meta property="og:type" content="article" />

<!-- Twitter meta -->
<meta name="twitter:title" content="How to explain gradient boosting">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@the_antlr_guy">
<meta name="twitter:creator" content="@the_antlr_guy">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="http://explained.ai/regularization/images/reg3D.svg">
<!-- END META -->
</head>
<body>
<div class="watermark">
<i><a href='http://explained.ai/regularization/index.html'>Main article</a><br>Brought to you by <a href='http://explained.ai'>explained.ai</a></i><br>
</div>

<h1>1 A visual explanation of regularization for linear models</h1>

<p></p>

<p><a href="http://parrt.cs.usfca.edu">Terence Parr</a></p>

<p style="font-size: 80%">(Terence teaches in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a>. You might know Terence as the creator of the ANTLR parser generator.)
<p>Please send comments, suggestions, or fixes to <a href="mailto:parrt@cs.usfca.edu">Terence</a>.</p>
</p>



<div id="toc">
<p class="toc_title">Contents</p>
<ul>
	<li><a href="#sec:1.1">Introduction</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.2">Review of linear regression</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.3">Motivation</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.4">The premise and trade-off of regularization</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.5">How regularization works conceptually</a>
	<ul>
			<li><a href="#sec:1.5.1">L2 Ridge regularization</a></li>
			<li><a href="#sec:1.5.2">L1 Lasso regularization</a></li>

	</ul>
	</li>
	<li><a href="#sec:1.6">The difference between L1 and L2 regularization</a>
	<ul>
			<li><a href="#sec:1.6.1">L1 regularization encourages zero coefficients</a></li>
			<li><a href="#sec:1.6.2">L1 and L2 regularization encourage zero coefficients for less predictive features</a></li>

	</ul>
	</li>
	<li><a href="#sec:1.7">How we implement L1 and L2 regularization</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.8">Resources</a>
	<ul>
	</ul>
	</li>

</ul>
</div>



<h2 id="sec:1.1">1.1 Introduction</h2>


<p>	<i>in progress</i></p>

<p>ADD LINKS TO IMAGES</p>

<p>Linear and logistic regression models are important because they are interpretable, fast, and form the basis of deep learning neural networks.  They are also extremely simple; we're just fitting lines (or hyperplanes) through training data. Unfortunately, linear models have a tendency to chase outliers in the training data, which often leads to models that don't generalize well to new data. To produce models that generalize better, we all know to <i>regularize</i> our models.  There are many forms of regularization, such as <i>early stopping</i> and <i>dropout</i> for deep learning, but for isolated linear models, <i>Lasso</i> and <i>Ridge</i> regularization are most common. (We'll call Lasso <i>L1</i> and Ridge <i>L2</i> for reasons that will become clear later.)</p>

<p>My goal in this article is to provide a visual explanation for regularization of linear models and to identify the critical stumbling block that makes it hard to understand how regularization works. Here it is upfront: <b>the math used to implement regularization does not correspond to the pictures everyone uses to explain regularization</b>. Take a look at the oft-copied picture on page 71 &ldquo;Shrinkage Methods&rdquo; from the excellent book <i>The Elements of Statistical Learning</i> (Hastie, Tibshirani, Friedman):</p>

<p><center>
<img src="images/ESL_reg.png" width="40%">
</center></p>

<p>Students see this multiple times in their careers but have trouble mapping that to the relatively straightforward mathematics used to regularize linear model training. The simple reason is that those graphs show how we  regularize models conceptually, with <i>hard constraints</i>, not how we actually implement regularization, with <i>soft constraints</i>! We'll go into that in detail shortly.  This single disconnect has no doubt caused untold amounts of consternation for those trying to deeply understand regularization. Read on to learn the real story.</p>

<p>I also want to answer key questions regarding L1 Lasso regularization: </p>
<ol>
<li><i>Does L1 encourage model coefficients to shrink to zero or does it simply not discourage zeroes?</i> (<b>It encourages zeros.</b>)</li>
<li><i>If L1 encourages zero coefficients, why/how does it do that?!</i> (<b>The answer requires a picture, see below.</b>)</li>
</ol>
<p>These are not easy questions to answer in detail, even for mathematicians. Try explaining simply, without handwaving, to an inquisitive and persistent student; you'll find that you're not exactly sure. ;)</p>

<p>Regularization for linear and logistic regression is done through the same penalty term in the loss function and so I will focus on just linear regression in this article.</p>



<h2 id="sec:1.2">1.2 Review of linear regression</h2>


<p>I'm assuming that readers more or less understand the mathematics of linear models and how we find optimal model coefficients to fit lines, but let's take a minute to review the important equations so we're all on the same page. (My plan is to keep mathematics notation to a minimum in this article though.)</p>

<div class="p_wrapper">
<span class=sidenote>	

<center>
<center>
<img src="images/ols.svg" width="80%">
</center>
</center>

<br><b>Figure 1.1</b>. Best fit line through 10 sample data points with ordinary least squares regression.</span>
<p class=p_left>A single-variable linear regression model is familiar to us from high school algebra: <img style="vertical-align: -3.0765pt;" src="images/eqn-D89137BDF4CCF6C6835408F177475EDB-depth002.93.svg">, where (coefficient) <img style="vertical-align: -3.0765pt;" src="images/eqn-5AF9E28D609B16EB25693F44EA9D7A8F-depth002.93.svg"> is the <span class=eqn>y</span>-intercept and <img style="vertical-align: -3.0765pt;" src="images/eqn-B4CEEC2C4656F5C1E7FC76C59C4F80F3-depth002.93.svg"> is the slope of the line.  For example, let's say we have the following 10 training records and want to draw the <i>best fit</i> line through the points, as shown in <b>Figure 1.1</b>. </p>
</div>
<div class="scrollbar_wrapper">
<table class="dataframe dataframe-indexed">
<thead>
	<tr><th>&nbsp;</th><th>x</th><th>y</th></tr>
    <tr><td></td></tr>
</thead>
<tbody>
	<tr>
	<td>0</td><td>0.0000</td><td>1.0539</td>
	</tr>
	<tr>
	<td>1</td><td>1.1111</td><td>1.6931</td>
	</tr>
	<tr>
	<td>2</td><td>2.2222</td><td>-0.0487</td>
	</tr>
	<tr>
	<td>3</td><td>3.3333</td><td>2.5201</td>
	</tr>
	<tr>
	<td>4</td><td>4.4444</td><td>4.9783</td>
	</tr>
	<tr>
	<td>5</td><td>5.5556</td><td>5.7886</td>
	</tr>
	<tr>
	<td>6</td><td>6.6667</td><td>7.0240</td>
	</tr>
	<tr>
	<td>7</td><td>7.7778</td><td>6.0265</td>
	</tr>
	<tr>
	<td>8</td><td>8.8889</td><td>9.5812</td>
	</tr>
	<tr>
	<td>9</td><td>10.0000</td><td>10.7626</td>
	</tr>
</tbody>
</table>
</div>
<p>The best fit line is <img style="vertical-align: -2.7825pt;" src="images/eqn-17A9D25CB66B4831A8C8C99788AF772F-depth002.65.svg">, where we use <img style="vertical-align: -2.7825pt;" src="images/eqn-5D28A7BA1A44A73B8C2ED21321697C59-depth002.65.svg"> to indicate it is an approximation of the true underlying relationship between <span class=eqn>x</span> and <span class=eqn>y</span>. Using Python and <span class=inlinecode>sklearn</span>, it's trivial to fit a linear model to this data:</p>


<div class="codeblk">lm = LinearRegression()        # Create a linear model
lm.fit(x, y)                   # Fit to x,y training data
</div>


<p>and get those optimal coefficients:</p>


<div class="codeblk">optimal_beta0 = lm.intercept_
optimal_beta1 = lm.coef_[0]
y_pred = lm.predict(x)         # What does model predict for x? (orange line)
</div>

<p class="stdout">optimal_beta0 = -0.170, optimal_beta1 = 1.022
y_pred [-0.17  0.97  2.1   3.24  4.37  5.51  6.64  7.78  8.91 10.05]
</p>

<div class="p_wrapper">
<span class=sidenote>	

<center>
<center>
<img src="images/ols_loss_3D.svg" width="80%">
</center>
</center>

<br><b>Figure 1.2</b>. Loss function in 3D where the black dot shows the smallest loss, the bottom of the bowl.</span>
<p class=p_left>The notion of <i>best fit</i> means choosing <img style="vertical-align: -3.0765pt;" src="images/eqn-5AF9E28D609B16EB25693F44EA9D7A8F-depth002.93.svg"> and <img style="vertical-align: -3.0765pt;" src="images/eqn-B4CEEC2C4656F5C1E7FC76C59C4F80F3-depth002.93.svg"> to minimize the average error, the average difference between the known true <img style="vertical-align: -2.7825pt;" src="images/eqn-4BE66AD2CF5C98540DB20BD7DF0C0413-depth002.65.svg"> values and the model predictions, <img style="vertical-align: -2.7825pt;" src="images/eqn-11477AE3C754D1E3F7950B3DAA7173A8-depth002.65.svg">.  To make things nice mathematically, and to avoid needing an absolute value operator, linear regression optimizes the average (mean) squared error.  That's where the term <i>Ordinary Least Squares</i> (OLS) comes from. The MSE function is a quadratic that always gives us a bowl shaped loss function, for 2 coefficients anyway, as shown in <b>Figure 1.2</b>.  For all <span class=eqn>n</span> training records <img style="vertical-align: -3.4125pt;" src="images/eqn-4D08776015650D52DEC654178D690689-depth003.25.svg">, we find <img style="vertical-align: -3.0765pt;" src="images/eqn-579638A364F763DB16AA52D12484F97E-depth002.93.svg"> to minimize:</p>
</div>

<div><img class="blkeqn" src="images/blkeqn-C3AF60FA16AA61331A4349C2BA1F55C3.svg" alt="" width=""></div>

<p>(If you're a <span class=inlinecode>numpy</span> junkie, that is just <span class=inlinecode>np.mean(y - m.predict(x))</span> for vectors <span class=inlinecode>y</span> and <span class=inlinecode>x</span>.) Plugging the model, our line equation, into that MSE we get:</p>

<div><img class="blkeqn" src="images/blkeqn-E04730F88F980C0BABD71A8F52E338B4.svg" alt="" width=""></div>

<div class="p_wrapper">
<span class=sidenote>	

<center>
<center>
<img src="images/ols_loss_2D.svg" width="70%">
</center>
</center>

<br><b>Figure 1.3</b>. Loss function contour projected onto 2D plane, as if we were looking from above.</span>
<p class=p_left>The loss function goes up as <img style="vertical-align: -3.0765pt;" src="images/eqn-5AF9E28D609B16EB25693F44EA9D7A8F-depth002.93.svg"> and <img style="vertical-align: -3.0765pt;" src="images/eqn-B4CEEC2C4656F5C1E7FC76C59C4F80F3-depth002.93.svg"> move away from the bottom of the bowl. The big black dot represents the minimum loss location, <img style="vertical-align: -3.4125pt;" src="images/eqn-2F106BCE53B4B8C2EEE6504AA6D750AB-depth003.25.svg"> = (-0.17, 1.022). (See <a href="code/loss3d.py">code/loss3d.py</a> for the code.) Three-dimensional plots are sometimes hard to interpret on a two-dimensional screen, so you will often see the loss function projected down onto the <img style="vertical-align: -3.0765pt;" src="images/eqn-5AF9E28D609B16EB25693F44EA9D7A8F-depth002.93.svg">, <img style="vertical-align: -3.0765pt;" src="images/eqn-B4CEEC2C4656F5C1E7FC76C59C4F80F3-depth002.93.svg"> plane, as shown in <b>Figure 1.3</b> (<a href="code/loss2d.py">code/loss2d.py</a>).</p>
</div>

<p>It's important to see the relationship between <b>Figure 1.1</b> and <b>Figure 1.2</b>. So, just to be clear, shifting <img style="vertical-align: -3.0765pt;" src="images/eqn-5AF9E28D609B16EB25693F44EA9D7A8F-depth002.93.svg"> and <img style="vertical-align: -3.0765pt;" src="images/eqn-B4CEEC2C4656F5C1E7FC76C59C4F80F3-depth002.93.svg"> in <b>Figure 1.2</b> causes the orange line in <b>Figure 1.1</b> to tilt or move up and down, away from the best fit.</p>



<h2 id="sec:1.3">1.3 Motivation</h2>


<p>So far so good.  Given some data, we can fit a best fit line through the data where &ldquo;best fit&rdquo; means the line that minimizes the average squared between true <span class=eqn>y</span> values and those predicted by the model. Now, let's tweak the last <span class=eqn>y</span> value to be about 10 times as big: </p>


<div class="codeblk">y.iloc[-1] = 100        # Make last y be an outlier 10x as big
lm = LinearRegression()
lm.fit(x, y)
y_pred = lm.predict(x)  # Get orange line points again
</div>

<p class="stdout">optimal_beta0 = -13.150, optimal_beta1 = 5.402
y_pred [-13.15  -7.15  -1.14   4.86  10.86  16.86  22.87  28.87  34.87  40.87]
</p>

<div class="p_wrapper">
<span class=sidenote>	

<center>
<center>
<img src="images/ols_outlier.svg" width="70%">
</center>
</center>

<br><b>Figure 1.4</b>. .</span>
<p class=p_left>Look what happens to the best (orange) fit line, as shown in <b>Figure 1.4</b>!  It has tilted substantially upwards towards the outlier. Because the loss function squares the error, an outlier can seriously distort the shape of the &ldquo;bowl&rdquo; and, hence, the minimum location of the optimal <img style="vertical-align: -3.0765pt;" src="images/eqn-5AF9E28D609B16EB25693F44EA9D7A8F-depth002.93.svg"> and <img style="vertical-align: -3.0765pt;" src="images/eqn-B4CEEC2C4656F5C1E7FC76C59C4F80F3-depth002.93.svg"> coefficients.  Instead of <img style="vertical-align: -3.0765pt;" src="images/eqn-6C00680F8C176B569E069D6E816E4F73-depth002.93.svg"> and <img style="vertical-align: -3.0765pt;" src="images/eqn-D8F652A1D2CB98714B882F419929EC92-depth002.93.svg">, the coefficients are <img style="vertical-align: -3.0765pt;" src="images/eqn-C401CA5408F5993D77C90DADA92BDBEB-depth002.93.svg"> and <img style="vertical-align: -3.0765pt;" src="images/eqn-2D2220E94D2A16F0EC672CB1A58217D3-depth002.93.svg">. All real data is noisy and sometimes outliers are common, which provides us with the motivation to regularize our linear models.</p>
</div>

<p>Let's try Ridge regularization. Using <span class=inlinecode>sklearn</span> again, we can fit a new line through the data using Ridge regression:</p>


<div class="codeblk">lm = Ridge(alpha=300)
lm.fit(x, y)
y_pred = lm.predict(x)
</div>

<p class="stdout">optimal_beta0 = 7.015, optimal_beta1 = 1.369
y_pred [ 7.02  8.54 10.06 11.58 13.1  14.62 16.14 17.67 19.19 20.71]
</p>

<div class="p_wrapper">
<span class=sidenote>	

<center>
<center>
<img src="images/ridge.svg" width="70%">
</center>
</center>

<br><b>Figure 1.5</b>. .</span>
<p class=p_left>The <span class=inlinecode>alpha=300</span> hyper parameter controls how much regularization we need, in this case a lot. (For those using TensorFlow's <span class=inlinecode>keras</span> interface, you might use something like <span class=inlinecode>activity_regularizer=regularizers.l2(300)</span> in one of your layers.)  While <span class=inlinecode>sklearn</span> uses <span class=inlinecode>alpha</span>, we will use <img style="vertical-align: -0.5pt;" src="images/eqn-C6A6EB61FD9C6C913DA73B3642CA147D-depth000.22.svg"> as the regularization hyper parameter as we get into the regularization penalty term of the loss function. Notice that the regularized slope, <img style="vertical-align: -3.0765pt;" src="images/eqn-68D1C99D9C4D5C191B6DADC852804EE9-depth002.93.svg">, is very close to the unregularized <img style="vertical-align: -3.0765pt;" src="images/eqn-D8F652A1D2CB98714B882F419929EC92-depth002.93.svg"> without the outlier. With regularization, the orange fitted line is held back to an appropriate angle, as shown in <b>Figure 1.5</b>.  Using Lasso regularization, <span class=inlinecode>Lasso(alpha=45).fit(x, y)</span>, we'd get similar results.</p>
</div>

<p>The price we pay for keeping the angle sane, is a less accurate (biased) overall result than we saw for the unregularized model for non-outlier data.  The regularized <span class=eqn>y</span>-intercept is larger, <img style="vertical-align: -3.0765pt;" src="images/eqn-A67A8E23D2CA74D0C01A386D6EE1B5D2-depth002.93.svg">, compared to unregularized <img style="vertical-align: -3.0765pt;" src="images/eqn-5AFCE1F68FDB6C7B005215AFB8C3950D-depth002.93.svg"> for the data without the outlier. You can see that the orange line rests above the majority of the data points instead of going through them. The outlier is still pulling the best fit line upward a little bit.</p>



<h2 id="sec:1.4">1.4 The premise and trade-off of regularization</h2>


<div class="p_wrapper">
<span class=sidenote>	

<center>
<center>
<img src="images/ames_ols.svg" width="100%">
</center>
</center>

<br><b>Figure 1.6</b>. OLS regression coefficients; data was normalized, dummy-encoded categorical variables.</span><span class=sidenote>	

<center>
<center>
<img src="images/ames_L2.svg" width="100%">
</center>
</center>

<br><b>Figure 1.7</b>. Ridge regression coefficients; data was  normalized, dummy-encoded categorical variables. 5-fold cross validation grid search used to identify best alpha value.</span>
<p class=p_left>We've motivated the need for regularization by showing how even a single outlier can seriously skew our model, but we still have no idea how regularization works.  Let's look at a real but small data set called <a href="http://jse.amstat.org/v19n3/decock.pdf">Ames housing price data</a> (<a href="code/ames.csv">ames.csv</a>) because it will point is in the direction of the solution. <b>Figure 1.6</b> shows a bar chart with one bar per coefficient using unregularized linear regression (with normalized explanatory variables and dummy-encoded categorical variables). Wow. Those are some big coefficients and, in fact, I had to clip them to slope magnitudes less than 1e8!  Contrast that with the Ridge-regularized coefficients in <b>Figure 1.7</b>, which are in a much more reasonable range. The accuracy of the unregularized model is ridiculously bad, with an error of <img style="vertical-align: -0.5pt;" src="images/eqn-A33F7BC1A73285E5C9C6CB7ED4150F16-depth000.29.svg"> dollars on a 20% validation set. Using Ridge regression, however, the error is only about $18k per house. With an average house price of about $180k, that's only 10% error on the same validation set. (If you're an R^2 fan, the regularized validation R^2 is 0.84.)</p>
</div>

<p>That gives us the clue we need to arrive at the premise of regularization: <b>extreme coefficients are unlikely to yield models that generalize well.</b> The solution, therefore, is simply to constrain the magnitude of linear model coefficients so they don't get too big.  Constraining the coefficients means not allowing them to reach their optimal position, at the minimum loss location.  That means we pay a price for improved generality in the form of decreased accuracy (increase in bias). Recall what we observed in <b>Figure 1.5</b> where the orange line sat a bit above the majority of the data. This is a worthwhile trade because, as we can see from this example, unregulated models on real data sets don't generalize well (they have terrible accuracy on validation sets).</p>



<h2 id="sec:1.5">1.5 How regularization works conceptually</h2>


<p>At this point, we've set the stage: Regularization is important for model generalization and the idea behind regularization is simply to constrain coefficients, at a small cost in overall accuracy.   Now, let's figure out how regularization constrains coefficients, at least conceptually. (In a later section, we'll look at how regularization actually works in practice.) </p>

<div class="p_wrapper">
<span class=sidenote>	

<center>
<center>
<img src="images/reg1D.svg" width="100%">
</center>
</center>

<br><b>Figure 1.8</b>. One coefficient regularization.</span>
<p class=p_left>Take a look at the hypothetical loss function in <b>Figure 1.8</b>, which is just <img style="vertical-align: -3.4125pt;" src="images/eqn-F676BFB939D5AC73868A23B1C49140A3-depth003.25.svg"> for some <img style="vertical-align: -3.0765pt;" src="images/eqn-B0603860FCFFE94E5B8EEC59ED813421-depth002.93.svg"> coefficient. (In terms of high school <span class=eqn>x</span> and <span class=eqn>y</span>, this is just <img style="vertical-align: -3.4125pt;" src="images/eqn-2583EA048B7B9A1B55EE8E1427528240-depth003.25.svg"> or &ldquo;a bowl shifted to 2&rdquo;.) The minimum loss is the bottom of the curve at <img style="vertical-align: -3.0765pt;" src="images/eqn-8762015EEC8EE2E02B9FBD2E154D1CD5-depth002.93.svg">. But, imagine we know that any coefficient bigger than 1.0 (or -1.0) will reduce generality. The best regularized coefficient is, therefore, <img style="vertical-align: -3.0765pt;" src="images/eqn-D2A73C8BC75872507040C3F5548BFFA2-depth002.93.svg">, which is on the constraint  boundary in the direction of the minimum. By &ldquo;best,&rdquo; we mean the closest we can get a coefficient to the loss function minimum location without exceeding our constraint.  </p>
</div>

<p>If the loss function minimum were on the other side of the vertical axis at, say, <img style="vertical-align: -3.0765pt;" src="images/eqn-B823D6864EF9D164C2576C90FD89814D-depth002.93.svg"> then the best regularized coefficient would be <img style="vertical-align: -3.0765pt;" src="images/eqn-FDB096CB419FC224D045C84EFFD1F398-depth002.93.svg">. We call this constraint of coefficients a <i>hard constraint</i> because the coefficients are strictly limited. (Foreshadowing, we actually implement these with soft constraints that just make bigger coefficients more and more expensive.) Mathematically, this hard constraint in one dimension is <img style="vertical-align: -3.4125pt;" src="images/eqn-94258FAC487B33485894E2D792F48792-depth003.25.svg">, but we could also use <img style="vertical-align: -3.0765pt;" src="images/eqn-580FADE10C9D152291E31DB20FF7A262-depth002.93.svg"> where <span class=eqn>t</span> represents the largest coefficient we want to allow (<img style="vertical-align: -0.5pt;" src="images/eqn-B73C3280B6F85A6AC520AF103083F535-depth000.14.svg"> in this case). In practice, we use a grid search to find the <span class=eqn>t</span> that gives the lowest validation error.</p>

<p>Now, let's consider the case where we have two coefficients to regularize instead of one (there are two variables in our explanatory matrix). Moving from one to two coefficients means the constraint line becomes a constraint region, and there are two common choices for the shape of this region. The first is a circle, which is used for Ridge, and the second is a diamond shape, which is used for Lasso. Ridge has some simpler properties, so let's take a look at it first.</p>



<h3 id="sec:1.5.1">1.5.1 L2 Ridge regularization</h3>


<div class="p_wrapper">
<span class=sidenote>	

<center>
<center>
<img src="images/reg3D.svg" width="115%">
</center>
</center>

<br><b>Figure 1.9</b>. Two coefficient regularization.</span>
<p class=p_left><b>Figure 1.9</b> shows a hypothetical loss function where the minimum is at <img style="vertical-align: -3.0765pt;" src="images/eqn-A65FD43D0A8BE064D1D9A97E58F7BD61-depth002.93.svg">, <img style="vertical-align: -3.0765pt;" src="images/eqn-04B4E93DDD30EEE1D8545252DB47DDAB-depth002.93.svg">.  Surrounding the origin (0,0) there is a circular hard constraint that would prevent coefficients from reaching the minimum loss function location. The  best we could do would be coefficients on the constraint circle in the direction of the loss function minimum.  Constraining two coefficients to a circle of radius <span class=eqn>r</span> surrounding the origin means constraining the length of vector (0,0) to <img style="vertical-align: -3.4125pt;" src="images/eqn-8E72519058455B362B43AE11EDE61322-depth003.25.svg"> to <span class=eqn>r</span>. The length of a vector is just the square root of the sum of the elements, <img style="vertical-align: -3.9689999pt;" src="images/eqn-8C7D741771511CFC8C13ACB5FB1CBB30-depth003.78.svg">. Or, we could get rid of the square root and pick some other constant, <span class=eqn>t</span>, for the constraint: <img style="vertical-align: -3.6225pt;" src="images/eqn-3792F6D1149F49F478C54FF6C1C22117-depth003.45.svg">. However we choose <span class=eqn>t</span>, summing the square of coefficients sweeps out a circle on the coefficient plane. As in the one-variable case, in practice, we use a grid search to find the <span class=eqn>t</span> that gives the minimum validation error; the  value of <span class=eqn>t</span> itself is not really meaningful to us.</p>
</div>
<div class=aside><b>Ridge = L2 regularization = weight decay</b><br>
Another way to say &ldquo;vector length&rdquo; is &ldquo;vector norm&rdquo; or &ldquo;Euclidean distance between two points.&rdquo; It turns out there are lots of norms and mathematicians classify them as <img style="vertical-align: -2.0475pt;" src="images/eqn-2C6F3B6C16DF97A1B00E04FF17E4906E-depth001.95.svg"> (which we'll see shortly), <img style="vertical-align: -2.0475pt;" src="images/eqn-07CBD6C155424E110559A84DF364BE5A-depth001.95.svg">, ..., <img style="vertical-align: -2.0475pt;" src="images/eqn-0CDB42D742B481E755BEEA25C80A0798-depth001.95.svg">. The Euclidean distance / vector length is called the <img style="vertical-align: -2.0475pt;" src="images/eqn-07CBD6C155424E110559A84DF364BE5A-depth001.95.svg"> norm and that is why we call Ridge &ldquo;<img style="vertical-align: -2.0475pt;" src="images/eqn-07CBD6C155424E110559A84DF364BE5A-depth001.95.svg"> regularization.&rdquo; The boundary is always at <span class=eqn>t</span> units away from the origin, sweeping out a circle. (See <a href="https://medium.com/@montjoile/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c">L1 vs L2 norm</a> for an easy to read discussion.)
<p>While we're talking about names, L2 regression was called &ldquo;Ridge&rdquo; in the <a href="https://www.math.arizona.edu/~hzhang/math574m/Read/RidgeRegressionBiasedEstimationForNonorthogonalProblems.pdf">original paper</a> from 1970 because the author remarked that surface plots of quadratic functions often look like ridges. </p>

<p>Also, L2 regularization (penalizing loss functions with sum of squares) is called <i>weight decay</i> in deep learning neural networks.</p>

</div>
<p>To get a feel for L2 regularization, look at the hypothetical loss functions in <b>Figure 1.10</b>, where I have projected the 3D loss &ldquo;bowl&rdquo; function onto the plane so we're looking at it from above. The big black dot indicates the (<img style="vertical-align: -3.0765pt;" src="images/eqn-0833BAFFE35058C08ED1280F777C109C-depth002.93.svg">) coordinate where the loss of function is minimum (bottom of the bowl). The big red dot is the (<img style="vertical-align: -3.0765pt;" src="images/eqn-0833BAFFE35058C08ED1280F777C109C-depth002.93.svg">) point on the boundary closest to the optimal location subject to the circular L2 constraint. </p>
<span class=figure><center>
<table style="">
<thead>
<tr>
<th align=center valign=top >(a)</th><th align=center valign=top >(b)</th><th align=center valign=top >(c)</th><th align=center valign=top >(d)</th>
</tr>
</thead>
<tbody>
<tr>
<td align=center valign=top>

<center>
<center>
<img src="images/l2-symmetric-0.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l2-symmetric-1.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l2-symmetric-2.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l2-symmetric-3.svg" width="100%">
</center>
</center>

</td>
</tr>
</tbody>
</table>
</center><center><b>Figure 1.10</b>. blort</center></span>
<p>All of these loss functions are symmetric, like your morning cereal bowl, which makes it easy to identify where we'd find the regularized (<img style="vertical-align: -3.0765pt;" src="images/eqn-0833BAFFE35058C08ED1280F777C109C-depth002.93.svg">) on the boundary circle. For symmetric loss functions, draw a line from the origin to the minimum loss function location, indicated by the dashed line in <b>Figure 1.10</b>. The optimal regularized (<img style="vertical-align: -3.0765pt;" src="images/eqn-0833BAFFE35058C08ED1280F777C109C-depth002.93.svg">) coefficients are at the intersection of that line and the boundary.  </p>

<p>Although I don't show it here, if the minimum loss function location sits within the boundary region, then the regularized location is exactly the same as the minimum loss location. Another special case is when the minimum loss location sits on one of the axes, as in <b>Figure 1.10</b> (b). One of the regularized coefficients will also sit on the axis and, hence, one of the coefficients will be zero. We will talk a lot about zero coefficients later when comparing Ridge and Lasso.</p>

<p>In general, the loss functions will not be symmetric, as depicted in <b>Figure 1.11</b>. Identifying the regularized coefficients on the boundary circle is not as simple as drawing a line between the origin and the minimum loss location, such as in <b>Figure 1.11</b> (a). (I'm sure in a non-Euclidean space, such as one warped by gravity, we could draw a &ldquo;straight&rdquo; line; is there an algebraic topologist in the house?) Now, the approaches to identify the location of minimum loss that sits directly on the boundary circle. That is exactly how I plotted the location of the red dots in these figures. I computed the loss function value at many points along the circle and simply identified the location where the loss function was the smallest. This is really important and so let's make a big deal out of it:</p>

<p><b>Finding the L2 coefficients location</b>: The L2 regularized coefficients sit on the L2 boundary circle where the loss function has the minimum value. So, just walk around the circle and identify the location with the minimum loss function value. (Unless the minimum loss location is inside the circle, in which case the regularized coefficient location is the same as the minimum loss location.)</p>

<p>I see instructors and articles recommend students look for where a loss function contour line touches the boundary region, but this can get you in trouble.  The contour maps are a 2D projection of a smooth 3D surface and so the number and location contour lines are kind of arbitrary. For example, I chose the number of contour lines in these plots, but I could've chosen one third as many, which would  likely make it impossible to find a contour line that intersected the boundary region. It's better to stick with finding the minimum loss location on the boundary circle and try not to get clever with visual rules.</p>
<span class=figure><center>
<table style="">
<thead>
<tr>
<th align=center valign=top >(a)</th><th align=center valign=top >(b)</th><th align=center valign=top >(c)</th><th align=center valign=top >(d)</th>
</tr>
</thead>
<tbody>
<tr>
<td align=center valign=top>

<center>
<center>
<img src="images/l2-0.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l2-1.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l2-2.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l2-3.svg" width="100%">
</center>
</center>

</td>
</tr>
</tbody>
</table>
</center><center><b>Figure 1.11</b>. blort</center></span>
<p>Let's look at the mathematics now. To regularize our MSE loss function is a simple matter of adding a &ldquo;subject to&rdquo; constraint to the definition, in this case, L2 norm <img style="vertical-align: -3.6225pt;" src="images/eqn-3792F6D1149F49F478C54FF6C1C22117-depth003.45.svg">:</p>

<div><img class="blkeqn" src="images/blkeqn-588B266459DC0293C0452BE900417E6C.svg" alt="" width=""></div>

<p>Everything to the left of &ldquo;subject to&rdquo; is identical to the unregularized (MSE) loss function, except we are using the two variable linear model: <img style="vertical-align: -3.0765pt;" src="images/eqn-291164F2A79A95B179514C4D9348C2CB-depth002.93.svg">. All we've done is to constrain how close (<img style="vertical-align: -3.0765pt;" src="images/eqn-0833BAFFE35058C08ED1280F777C109C-depth002.93.svg">) coefficients can get to the loss function minimum location.  Note that we do not constrain the <span class=eqn>y</span>-intercept <img style="vertical-align: -3.0765pt;" src="images/eqn-5AF9E28D609B16EB25693F44EA9D7A8F-depth002.93.svg"> (see page 64 in &ldquo;<i>The elements of statistical learning</i>&rdquo;). We are only concerned with constraining slope angles, not where the line touches the <span class=eqn>y</span>-axis. We find the value of <span class=eqn>t</span> using a brute force search that minimizes the validation error.</p>

<p>That's all there is to the concept of regularization: adding a hard constraint to the loss function equation. Congratulations, if you've gotten this far and understood everything! The only remaining wrinkle is how we actually implement this (see below).</p>

<p>Now, let's take a look at the other common form of regularization.</p>



<h3 id="sec:1.5.2">1.5.2 L1 Lasso regularization</h3>


<p>If we use a diamond shape rather than a circle around the origin as the boundary region, we get <i>Lasso regularization</i>, which we will call L1 regularization because it constrains <img style="vertical-align: -3.0765pt;" src="images/eqn-F543C88F1D1D45B3C49D49DBE3828B6F-depth002.93.svg"> coefficients using the L1 norm. The L1 norm gives us a diamond shape, obtained by constraining the sum of coefficient magnitudes to some constant, <span class=eqn>t</span>.  Lasso stands for &ldquo;Least Absolute Shrinkage and Selection Operator,&rdquo; according to the <a href="http://www-stat.stanford.edu/~tibs/lasso/lasso.pdf">original paper</a>. Why we would choose a diamond over a circle will become clear shortly.</p>

<p>The plots in <b>Figure 1.12</b> show the L1 diamond constraint regions in the special case where the loss function is symmetric. The regularized (<img style="vertical-align: -3.0765pt;" src="images/eqn-0833BAFFE35058C08ED1280F777C109C-depth002.93.svg">) coefficient location on the diamond occurs where a perpendicular line emanates from the diamond to the minimum loss location. The dotted lines in <b>Figure 1.12</b> show these perpendicular lines. Contrast this with L2 regularization, which draws a line from the origin to the minimum loss location for symmetric loss functions.</p>
<span class=figure><center>
<table style="">
<thead>
<tr>
<th align=center valign=top >(a)</th><th align=center valign=top >(b)</th><th align=center valign=top >(c)</th><th align=center valign=top >(d)</th>
</tr>
</thead>
<tbody>
<tr>
<td align=center valign=top>

<center>
<center>
<img src="images/l1-symmetric-0.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l1-symmetric-1.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l1-symmetric-2.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l1-symmetric-3.svg" width="100%">
</center>
</center>

</td>
</tr>
</tbody>
</table>
</center><center><b>Figure 1.12</b>.  boundary distance from origin is some of coefficient magnitudes less than some constant, t.</center></span>
<p>In <b>Figure 1.13</b>, you'll see the general case where the loss functions are asymmetric. One of the key takeaways from these examples is that three out of four loss functions have a zero coefficient (the red dot is on an axis at a diamond peak). This is despite the fact that the minimum loss function locations look to be nowhere near an axis, which brings us to the difference between L1 and L2 in a nutshell: <b>L1 tends to give a lot more zero coefficients than L2</b>.</p>
<span class=figure><center>
<table style="">
<thead>
<tr>
<th align=center valign=top >(a)</th><th align=center valign=top >(b)</th><th align=center valign=top >(c)</th><th align=center valign=top >(d)</th>
</tr>
</thead>
<tbody>
<tr>
<td align=center valign=top>

<center>
<center>
<img src="images/l1-0.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l1-1.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l1-2.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l1-3.svg" width="100%">
</center>
</center>

</td>
</tr>
</tbody>
</table>
</center><center><b>Figure 1.13</b>. blort</center></span>
<p>To find regularized coefficients, we follow the same rule we did for L2, except using a different boundary shape:</p>

<p><b>Finding the L1 coefficients location</b>: The L1 regularized coefficients sit on the L1 boundary diamond where the loss function has the minimum value. So, just walk around the diamond and identify the location with the minimum loss function value. (Unless the minimum loss location is inside the diamond, in which case the regularized coefficient location is the same as the minimum loss location.)</p>

<p>Just as we did for L2, regularizing the loss function means adding a &ldquo;subject to&rdquo; constraint. The only difference is that we are summing the coefficient magnitudes (absolute values) rather than the squared coefficient values:</p>

<div><img class="blkeqn" src="images/blkeqn-01E523C82FF3C0B5652153D0FB522443.svg" alt="" width=""></div>

<p>As before, we don't care what the value of <span class=eqn>t</span> is <i>per se</i>; we find the <span class=eqn>t</span> through brute force that gives the lowest validation error.</p>

<p>If you've made it this far, you now understand exactly how L1 Lasso and L2 Ridge regularization work conceptually. The next step is to compare the two in detail and then we'll be ready to describe the actual implementation of regularization (versus the conceptual mechanism we've seen so far).</p>
<div class=aside><b>The Ridge and Lasso terms seem backwards</b><br>
In case you haven't noticed, the Ridge and Lasso terms seem kind of backwards. The circular boundary of Ridge looks more like a lasso and the constraint region for Lasso, with lots of pointy discontinuities, looks like a bunch of ridges. Ha! For more good-natured teasing of statisticians, see <a href="https://explained.ai/statspeak/index.html">Statisticians say the darndest things</a>.
</div>



<h2 id="sec:1.6">1.6 The difference between L1 and L2 regularization</h2>


<p>If both of these regularization techniques work well, you might be wondering why we need both. It turns out they have different but equally useful properties. From a practical standpoint, L1 tends to shrink coefficients to zero whereas L2 tends to shrink coefficients evenly. L1 is therefore useful for feature selection, as we can drop any variables associated with coefficients that go to zero. L2, on the other hand, is useful when you have collinear/codependent features.  Codependence tends to increase coefficient variance, making coefficients unreliable/unstable, which hurts model  generality. L2 reduces the variance of these estimates, which counteracts the effect of codependencies.</p>



<h3 id="sec:1.6.1">1.6.1 L1 regularization encourages zero coefficients</h3>


<p>One of the key questions that I want to answer is: &ldquo;<i>Does L1 encourage model coefficients to shrink to zero?</i>&rdquo; (The answer is, Yes!) So, let's do some two-variable simulations of random quadratic loss functions at random locations and see how many end up with a coefficient at zero. There is no guarantee that these random paraboloid loss functions in any way represent real data sets, but it's a way to at least compare L1 and L2 regularization. Let's start out with symmetric loss functions that look like bowls of various  sizes and locations. </p>
<center>
<table style="">
<thead>
</thead>
<tbody>
<tr>
<td align=center valign=top>

<center>
<center>
<img src="images/l1-symmetric-cloud.png" width="60%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l2-symmetric-cloud.png" width="60%">
</center>
</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>As you can see in the simulations (6000 trials), the L1 diamond constraint zeros a coefficient for any loss function whose minimum is in the zone perpendicular to the diamond edges. The L2 circular constraint only zeros a coefficient for loss function minimums on one of the axes. The gradation from green to purple represents the minimum distance of a coefficient pair on the circle to any of the coefficient zeros (North, South, East, West compass points). Clearly, L1 gives many more zero coefficients (66%) than L2 (3%) for symmetric loss functions.</p>

<p>Here's what the same simulation looks like for general (potentially asymmetric and at an angle) loss functions:</p>
<center>
<table style="">
<thead>
</thead>
<tbody>
<tr>
<td align=center valign=top>

<center>
<center>
<img src="images/l1-cloud.png" width="60%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l2-cloud.png" width="60%">
</center>
</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>Because of the various angles and shapes, such as we saw in <b>Figure 1.11</b>, more of the regularized coefficients for both L1 (73%) and L2 (5%) constraints become zero. So this basically answers the question: Yes, L1 regularized coefficients are much more likely to become zeros than L2 coefficients.</p>



<h3 id="sec:1.6.2">1.6.2 L1 and L2 regularization encourage zero coefficients for less predictive features</h3>


<p>On the other hand, we actually want to answer a more specific question: &ldquo;<i>Does L1 encourage zero coefficients for less predictive or useless features?</i>&rdquo; (Yep!) To answer that, we need to know what  loss functions look like for less predictive features. Imagine one feature is pretty important and the other is not. That would imply that the loss function looks like a taco shell or canoe, and at 90 degrees to one of the axes. <b>Figure 1.14</b> shows some examples for the L1 constraint. If <img style="vertical-align: -3.0765pt;" src="images/eqn-B4CEEC2C4656F5C1E7FC76C59C4F80F3-depth002.93.svg"> is not very predictive as in subfigure (c), then movement left and right does not increase the cost very much, whereas, moving up and down costs a lot (we're crossing a lot of contour lines).</p>
<span class=figure><center>
<table style="">
<thead>
<tr>
<th align=center valign=top >(a)</th><th align=center valign=top >(b)</th><th align=center valign=top >(c)</th><th align=center valign=top >(d)</th>
</tr>
</thead>
<tbody>
<tr>
<td align=center valign=top>

<center>
<center>
<img src="images/l1-orthogonal-0.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l1-orthogonal-1.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l1-orthogonal-2.svg" width="100%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l1-orthogonal-3.svg" width="100%">
</center>
</center>

</td>
</tr>
</tbody>
</table>
</center><center><b>Figure 1.14</b>. blort</center></span>
<p>With the shape of those orthogonal loss functions in mind, let's do another simulation and see how many regularized coefficients go to zero:</p>
<center>
<table style="">
<thead>
</thead>
<tbody>
<tr>
<td align=center valign=top>

<center>
<center>
<img src="images/l1-orthogonal-cloud.png" width="60%">
</center>
</center>

</td><td align=center valign=top>

<center>
<center>
<img src="images/l2-orthogonal-cloud.png" width="60%">
</center>
</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>Orthogonal loss functions result in more zero coefficients than the general case, which is what we would expect, but the effect is not huge; 73% to 80%. L2, on the other hand, sees a huge boost in the number of zero coefficients, from 5% to 43%!  We definitely want more zero coefficients for the case where one of the features is less predictive.  Fortunately, both L1 and L2 deliver in this case! </p>

<p>A more scientific approach would also do simulations for the many variable case, but I think this article provides enough evidence for me to believe L1 encourages zeros. Besides, James D. Wilson, a statistician and fellow faculty member, told me there's a theorem that says that the probability of a coefficient going to zero approaches 100% as the number of features goes to infinity. Apparently, as the number of features goes to infinity, the diamond-shaped collapses in on itself to a point.</p>
<div class=aside><b>Random loss functions used in simulation</b><br>
For math nerds, this is the equation used to generate the random loss functions:
<div><img class="blkeqn" src="images/blkeqn-71F4C9E9A5474302445E4AD6322B194C.svg" alt="" width=""></div>

<p>where <img style="vertical-align: -3.4125pt;" src="images/eqn-FE9FB5B1A0E8DC9AFD7B356D4C55A611-depth003.25.svg"> scales the bowl in the <img style="vertical-align: -3.0765pt;" src="images/eqn-B4CEEC2C4656F5C1E7FC76C59C4F80F3-depth002.93.svg"> direction, <img style="vertical-align: -3.4125pt;" src="images/eqn-F40AC17B6189D537B1D28D0EDECE9FC3-depth003.25.svg"> scales the <img style="vertical-align: -3.0765pt;" src="images/eqn-D9F51E864A6151F57E727294DA7AC28C-depth002.93.svg"> direction, <img style="vertical-align: -3.4125pt;" src="images/eqn-DD69012E53197BE3398DD058A5A6720C-depth003.25.svg"> controls the amount of tilt/angle away from vertical or horizontal, and (<img style="vertical-align: -3.4125pt;" src="images/eqn-E13E31C2E04DA324A2975421D9B029C3-depth003.25.svg">,<img style="vertical-align: -3.4125pt;" src="images/eqn-796D8E5EFBD5F5FC065D9B0250C0AA78-depth003.25.svg">) is the position in coefficient space of the minimum loss function value. (<img style="vertical-align: -3.4125pt;" src="images/eqn-284297B52391FDE0C273B9B2B34F89EC-depth003.25.svg"> means uniform random variable between <span class=eqn>k</span> and <span class=eqn>l</span>.)</p>

</div>



<h2 id="sec:1.7">1.7 How we implement L1 and L2 regularization</h2>


<p>	foo</p>

<p><b>Acknowledgements</b>. I'd like to thank mathematicians Steve Devlin and David Uminsky, also faculty in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a>, for helping me understand the mathematics and why L1 regularization encourages zero coefficients.</p>



<h2 id="sec:1.8">1.8 Resources</h2>


<p>My MSDS621 project <a href="https://github.com/parrt/msds621/raw/master/projects/linreg/linreg.pdf">Using gradient descent to fit regularized linear models</a></p>

<p>	<a href="https://www.cs.princeton.edu/courses/archive/spring16/cos495/slides/DL_lecture3_regularization_I.pdf">Deep Learning Basics Lecture 3: Regularization I (slides)</a> by Yingyu Liang at  Princeton University.</p>

<p><a href="https://uc-r.github.io/regularized_regression">Regularized Regression</a> from University of Cincinnati.</p>

<p><a href="https://arxiv.org/pdf/1509.09169.pdf">Lecture notes on ridge regression</a> by Wessel N. van Wieringen.</p>

<p><a href="https://www.math.arizona.edu/~hzhang/math574m/Read/RidgeRegressionBiasedEstimationForNonorthogonalProblems.pdf">Ridge Regression: Biased Estimation for Nonorthogonal Problems</a> by Hoerl and Kennard, Journal Technometrics, 1970.</p>

<p><a href="http://www-stat.stanford.edu/~tibs/lasso/lasso.pdf">Regression Shrinkage and Selection via the Lasso</a> by Tibshirani in Journal of the Royal Statistical Society, 1996.</p>



</body>
</html>